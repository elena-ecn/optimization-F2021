{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"DesignOptimization_HW_3.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"harmful-logging"},"source":["### Problem 1 (50 points) \n","\n","Vapor-liquid equilibria data are correlated using two adjustable parameters $A_{12}$ and $A_{21}$ per binary\n","mixture. For low pressures, the equilibrium relation can be formulated as:\n","\n","$$\n","\\begin{aligned}\n","p = & x_1\\exp\\left(A_{12}\\left(\\frac{A_{21}x_2}{A_{12}x_1+A_{21}x_2}\\right)^2\\right)p_{water}^{sat}\\\\\n","& + x_2\\exp\\left(A_{21}\\left(\\frac{A_{12}x_1}{A_{12}x_1+A_{21}x_2}\\right)^2\\right)p_{1,4 dioxane}^{sat}.\n","\\end{aligned}\n","$$\n","\n","Here the saturation pressures are given by the Antoine equation\n","\n","$$\n","\\log_{10}(p^{sat}) = a_1 - \\frac{a_2}{T + a_3},\n","$$\n","\n","where $T = 20$($^{\\circ}{\\rm C}$) and $a_{1,2,3}$ for a water - 1,4 dioxane\n","system is given below.\n","\n","|             | $a_1$     | $a_2$      | $a_3$     |\n","|:------------|:--------|:---------|:--------|\n","| Water       | 8.07131 | 1730.63  | 233.426 |\n","| 1,4 dioxane | 7.43155 | 1554.679 | 240.337 |\n","\n","\n","The following table lists the measured data. Recall that in a binary system $x_1 + x_2 = 1$.\n","\n","|$x_1$ | 0.0 | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 | 0.6 | 0.7 | 0.8 | 0.9 | 1.0 |\n","|:-----|:--------|:---------|:--------|:-----|:-----|:-----|:-----|:-----|:-----|:-----|:-----|\n","|$p$| 28.1 | 34.4 | 36.7 | 36.9 | 36.8 | 36.7 | 36.5 | 35.4 | 32.9 | 27.7 | 17.5 |\n","\n","Estimate $A_{12}$ and $A_{21}$ using data from the above table: \n","\n","1. Formulate the least square problem; \n","2. Since the model is nonlinear, the problem does not have an analytical solution. Therefore, solve it using the gradient descent or Newton's method implemented in HW1; \n","3. Compare your optimized model with the data. Does your model fit well with the data?\n","\n","\n"],"id":"harmful-logging"},{"cell_type":"markdown","metadata":{"id":"ydz7pbTxsa_X"},"source":["#### Solution \n","\n","1.\n","The given problem can be formulated as a Least Squares problem as follows:\n","\n","$$\n","\\begin{aligned}\n"," \\underset{A_{12},A_{21}}{\\text{minimize: }} \\sum_{i=1}^{N=11}(p(x_i;A_{12},A_{21})-p_i)^2\n","\\end{aligned}\n","$$\n","\n","<br>\n","2.\n","\n","Saturation pressure for water:\n","\n","$$\n","\\log_{10}(p_{water}^{sat}) = a_1 - \\frac{a_2}{T + a_3} = 8.07131 - \\frac{1730.63}{20 + 233.426} = 1.24237\n","$$\n","\n","<br>\n","$$\n","\\Rightarrow p_{water}^{sat} = 17.47325\n","$$\n","\n","\n","Saturation pressure for 1,4dioxane:\n","\n","$$\n","\\log_{10}(p_{1,4dioxane}^{sat}) = a_1 - \\frac{a_2}{T + a_3} = 7.43155 - \\frac{1554.679}{20 + 240.337} = 1.45975\n","$$\n","\n","<br> \t\n","$$\n","\\Rightarrow p_{water}^{sat} = 28.82409\n","$$\n","\n","<br>\n","With $x_2=1-x_1$, the equilibrium relation becomes:\n","\n","$$\n","\\begin{aligned}\n","p = & x_1\\exp\\left(A_{12}\\left(\\frac{A_{21}(1-x_1)}{A_{12}x_1+A_{21}(1-x_1)}\\right)^2\\right)p_{water}^{sat}\\\\\n","& + (1-x_1)\\exp\\left(A_{21}\\left(\\frac{A_{12}x_1}{A_{12}x_1+A_{21}(1-x_1)}\\right)^2\\right)p_{1,4 dioxane}^{sat}.\n","\\end{aligned}\n","$$\n"],"id":"ydz7pbTxsa_X"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AsR35UNvsX7C","executionInfo":{"status":"ok","timestamp":1633102613825,"user_tz":-180,"elapsed":10501,"user":{"displayName":"Elena Oikonomou","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdjieaR8SwyW3oT5Gj_-eTKl6SVjziMwjwjye6=s64","userId":"03571600125738632827"}},"outputId":"7918efb9-1403-4363-c82a-2b5d071138a3"},"source":["import numpy as np\n","import torch \n","\n","\n","\n","def saturation_pressure(T, a1, a2, a3):\n","    \"\"\"Compute the saturation pressure.\"\"\"\n","\n","    log_p = a1 - (a2/(T + a3))\n","    p = 10**log_p\n","    return p\n","\n","\n","def gradient_descent(x1, p, a, eps, max_iter):\n","    \"\"\" \"\"\"\n","\n","    # Randomly initialize weights\n","    # A12 = torch.randn((), requires_grad=True, device=device, dtype=torch.float) \n","    # A21 = torch.randn((), requires_grad=True, device=device, dtype=torch.float) \n","\n","    A = torch.randn(2, requires_grad=True, device=device, dtype=torch.float)\n","\n","    error = float('inf')\n","    iter = 0\n","\n","    while error > eps and iter < max_iter:\n","\n","        # Compute p prediction (Forward pass)\n","        p_pred = x1*torch.exp(A[0]*(A[1]*(1-x1)/(A[0]*x1+A[1]*(1-x1)))**2)*water_sat_p + \\\n","                (1-x1)*torch.exp(A[1]*(A[0]*x1/(A[0]*x1+A[1]*(1-x1)))**2)*dioxane_sat_p\n","        \n","        # Compute loss\n","        loss = (p_pred - p).pow(2).sum()\n","\n","        # Print loss every 100 iterations\n","        if iter % 100 == 0:\n","            print('Iteration: {}, Loss: {} '.format(iter, loss.item()))\n","        \n","        # Compute gradient of the loss wrt all the learnable parameters of the model\n","        loss.backward()\n","\n","        # Current error\n","        error = torch.linalg.norm(A)          \n","\n","\n","        # Update the weights using Gradient Descent\n","        with torch.no_grad():  # We don't need the GD algorithm itself to be differentiable wrt A\n","            A -= a * A.grad\n","        \n","            # Clear the gradients (so they don't accumulate)\n","            A.grad.zero_()\n","        \n","        \n","        iter += 1\n","    \n","    print('\\nNumber of iterations: {}'.format(iter))\n","    print('Final loss value: {}'.format(loss.data.numpy()))\n","    print('Final error: {}'.format(error))\n","\n","    return A.data\n","\n","\n","\n","\n","\n","if __name__ == '__main__':\n","\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","    # Parameters\n","    # --------------------------------------------------------------------------\n","    eps = 1e-3                       # Tolerance (termination criterion)\n","    max_iter = 20000                 # Maximum iterations before halting\n","\n","    # Compute saturation pressures\n","    # --------------------------------------------------------------------------\n","    water_sat_p = saturation_pressure(T=20, a1=8.07131, a2=1730.63, a3=233.426)\n","    dioxane_sat_p = saturation_pressure(T=20, a1=7.43155, a2=1554.679, a3=240.337)\n","\n","\n","    \n","    # Input data\n","    x1 = torch.tensor([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], device=device, dtype=torch.float)\n","    # Output data\n","    p = torch.tensor([28.1, 34.4, 36.7, 36.9, 36.8, 36.7, 36.5, 35.4, 32.9, 27.7, 17.5], device=device, dtype=torch.float)\n","\n","    sol = gradient_descent(x1, p, 0.001, eps, max_iter)\n","\n","\n","    print('The solution is: A = {}'.format(sol.data.numpy()))\n","\n","    \n","\n","\n"],"id":"AsR35UNvsX7C","execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration: 0, Loss: 1536.2203369140625 \n","Iteration: 100, Loss: 0.6701919436454773 \n","Iteration: 200, Loss: 0.6701900362968445 \n","Iteration: 300, Loss: 0.6701900362968445 \n","Iteration: 400, Loss: 0.6701900362968445 \n","Iteration: 500, Loss: 0.6701900362968445 \n","Iteration: 600, Loss: 0.6701900362968445 \n","Iteration: 700, Loss: 0.6701900362968445 \n","Iteration: 800, Loss: 0.6701900362968445 \n","Iteration: 900, Loss: 0.6701900362968445 \n","Iteration: 1000, Loss: 0.6701900362968445 \n","Iteration: 1100, Loss: 0.6701900362968445 \n","Iteration: 1200, Loss: 0.6701900362968445 \n","Iteration: 1300, Loss: 0.6701900362968445 \n","Iteration: 1400, Loss: 0.6701900362968445 \n","Iteration: 1500, Loss: 0.6701900362968445 \n","Iteration: 1600, Loss: 0.6701900362968445 \n","Iteration: 1700, Loss: 0.6701900362968445 \n","Iteration: 1800, Loss: 0.6701900362968445 \n","Iteration: 1900, Loss: 0.6701900362968445 \n","Iteration: 2000, Loss: 0.6701900362968445 \n","Iteration: 2100, Loss: 0.6701900362968445 \n","Iteration: 2200, Loss: 0.6701900362968445 \n","Iteration: 2300, Loss: 0.6701900362968445 \n","Iteration: 2400, Loss: 0.6701900362968445 \n","Iteration: 2500, Loss: 0.6701900362968445 \n","Iteration: 2600, Loss: 0.6701900362968445 \n","Iteration: 2700, Loss: 0.6701900362968445 \n","Iteration: 2800, Loss: 0.6701900362968445 \n","Iteration: 2900, Loss: 0.6701900362968445 \n","Iteration: 3000, Loss: 0.6701900362968445 \n","Iteration: 3100, Loss: 0.6701900362968445 \n","Iteration: 3200, Loss: 0.6701900362968445 \n","Iteration: 3300, Loss: 0.6701900362968445 \n","Iteration: 3400, Loss: 0.6701900362968445 \n","Iteration: 3500, Loss: 0.6701900362968445 \n","Iteration: 3600, Loss: 0.6701900362968445 \n","Iteration: 3700, Loss: 0.6701900362968445 \n","Iteration: 3800, Loss: 0.6701900362968445 \n","Iteration: 3900, Loss: 0.6701900362968445 \n","Iteration: 4000, Loss: 0.6701900362968445 \n","Iteration: 4100, Loss: 0.6701900362968445 \n","Iteration: 4200, Loss: 0.6701900362968445 \n","Iteration: 4300, Loss: 0.6701900362968445 \n","Iteration: 4400, Loss: 0.6701900362968445 \n","Iteration: 4500, Loss: 0.6701900362968445 \n","Iteration: 4600, Loss: 0.6701900362968445 \n","Iteration: 4700, Loss: 0.6701900362968445 \n","Iteration: 4800, Loss: 0.6701900362968445 \n","Iteration: 4900, Loss: 0.6701900362968445 \n","Iteration: 5000, Loss: 0.6701900362968445 \n","Iteration: 5100, Loss: 0.6701900362968445 \n","Iteration: 5200, Loss: 0.6701900362968445 \n","Iteration: 5300, Loss: 0.6701900362968445 \n","Iteration: 5400, Loss: 0.6701900362968445 \n","Iteration: 5500, Loss: 0.6701900362968445 \n","Iteration: 5600, Loss: 0.6701900362968445 \n","Iteration: 5700, Loss: 0.6701900362968445 \n","Iteration: 5800, Loss: 0.6701900362968445 \n","Iteration: 5900, Loss: 0.6701900362968445 \n","Iteration: 6000, Loss: 0.6701900362968445 \n","Iteration: 6100, Loss: 0.6701900362968445 \n","Iteration: 6200, Loss: 0.6701900362968445 \n","Iteration: 6300, Loss: 0.6701900362968445 \n","Iteration: 6400, Loss: 0.6701900362968445 \n","Iteration: 6500, Loss: 0.6701900362968445 \n","Iteration: 6600, Loss: 0.6701900362968445 \n","Iteration: 6700, Loss: 0.6701900362968445 \n","Iteration: 6800, Loss: 0.6701900362968445 \n","Iteration: 6900, Loss: 0.6701900362968445 \n","Iteration: 7000, Loss: 0.6701900362968445 \n","Iteration: 7100, Loss: 0.6701900362968445 \n","Iteration: 7200, Loss: 0.6701900362968445 \n","Iteration: 7300, Loss: 0.6701900362968445 \n","Iteration: 7400, Loss: 0.6701900362968445 \n","Iteration: 7500, Loss: 0.6701900362968445 \n","Iteration: 7600, Loss: 0.6701900362968445 \n","Iteration: 7700, Loss: 0.6701900362968445 \n","Iteration: 7800, Loss: 0.6701900362968445 \n","Iteration: 7900, Loss: 0.6701900362968445 \n","Iteration: 8000, Loss: 0.6701900362968445 \n","Iteration: 8100, Loss: 0.6701900362968445 \n","Iteration: 8200, Loss: 0.6701900362968445 \n","Iteration: 8300, Loss: 0.6701900362968445 \n","Iteration: 8400, Loss: 0.6701900362968445 \n","Iteration: 8500, Loss: 0.6701900362968445 \n","Iteration: 8600, Loss: 0.6701900362968445 \n","Iteration: 8700, Loss: 0.6701900362968445 \n","Iteration: 8800, Loss: 0.6701900362968445 \n","Iteration: 8900, Loss: 0.6701900362968445 \n","Iteration: 9000, Loss: 0.6701900362968445 \n","Iteration: 9100, Loss: 0.6701900362968445 \n","Iteration: 9200, Loss: 0.6701900362968445 \n","Iteration: 9300, Loss: 0.6701900362968445 \n","Iteration: 9400, Loss: 0.6701900362968445 \n","Iteration: 9500, Loss: 0.6701900362968445 \n","Iteration: 9600, Loss: 0.6701900362968445 \n","Iteration: 9700, Loss: 0.6701900362968445 \n","Iteration: 9800, Loss: 0.6701900362968445 \n","Iteration: 9900, Loss: 0.6701900362968445 \n","Iteration: 10000, Loss: 0.6701900362968445 \n","Iteration: 10100, Loss: 0.6701900362968445 \n","Iteration: 10200, Loss: 0.6701900362968445 \n","Iteration: 10300, Loss: 0.6701900362968445 \n","Iteration: 10400, Loss: 0.6701900362968445 \n","Iteration: 10500, Loss: 0.6701900362968445 \n","Iteration: 10600, Loss: 0.6701900362968445 \n","Iteration: 10700, Loss: 0.6701900362968445 \n","Iteration: 10800, Loss: 0.6701900362968445 \n","Iteration: 10900, Loss: 0.6701900362968445 \n","Iteration: 11000, Loss: 0.6701900362968445 \n","Iteration: 11100, Loss: 0.6701900362968445 \n","Iteration: 11200, Loss: 0.6701900362968445 \n","Iteration: 11300, Loss: 0.6701900362968445 \n","Iteration: 11400, Loss: 0.6701900362968445 \n","Iteration: 11500, Loss: 0.6701900362968445 \n","Iteration: 11600, Loss: 0.6701900362968445 \n","Iteration: 11700, Loss: 0.6701900362968445 \n","Iteration: 11800, Loss: 0.6701900362968445 \n","Iteration: 11900, Loss: 0.6701900362968445 \n","Iteration: 12000, Loss: 0.6701900362968445 \n","Iteration: 12100, Loss: 0.6701900362968445 \n","Iteration: 12200, Loss: 0.6701900362968445 \n","Iteration: 12300, Loss: 0.6701900362968445 \n","Iteration: 12400, Loss: 0.6701900362968445 \n","Iteration: 12500, Loss: 0.6701900362968445 \n","Iteration: 12600, Loss: 0.6701900362968445 \n","Iteration: 12700, Loss: 0.6701900362968445 \n","Iteration: 12800, Loss: 0.6701900362968445 \n","Iteration: 12900, Loss: 0.6701900362968445 \n","Iteration: 13000, Loss: 0.6701900362968445 \n","Iteration: 13100, Loss: 0.6701900362968445 \n","Iteration: 13200, Loss: 0.6701900362968445 \n","Iteration: 13300, Loss: 0.6701900362968445 \n","Iteration: 13400, Loss: 0.6701900362968445 \n","Iteration: 13500, Loss: 0.6701900362968445 \n","Iteration: 13600, Loss: 0.6701900362968445 \n","Iteration: 13700, Loss: 0.6701900362968445 \n","Iteration: 13800, Loss: 0.6701900362968445 \n","Iteration: 13900, Loss: 0.6701900362968445 \n","Iteration: 14000, Loss: 0.6701900362968445 \n","Iteration: 14100, Loss: 0.6701900362968445 \n","Iteration: 14200, Loss: 0.6701900362968445 \n","Iteration: 14300, Loss: 0.6701900362968445 \n","Iteration: 14400, Loss: 0.6701900362968445 \n","Iteration: 14500, Loss: 0.6701900362968445 \n","Iteration: 14600, Loss: 0.6701900362968445 \n","Iteration: 14700, Loss: 0.6701900362968445 \n","Iteration: 14800, Loss: 0.6701900362968445 \n","Iteration: 14900, Loss: 0.6701900362968445 \n","Iteration: 15000, Loss: 0.6701900362968445 \n","Iteration: 15100, Loss: 0.6701900362968445 \n","Iteration: 15200, Loss: 0.6701900362968445 \n","Iteration: 15300, Loss: 0.6701900362968445 \n","Iteration: 15400, Loss: 0.6701900362968445 \n","Iteration: 15500, Loss: 0.6701900362968445 \n","Iteration: 15600, Loss: 0.6701900362968445 \n","Iteration: 15700, Loss: 0.6701900362968445 \n","Iteration: 15800, Loss: 0.6701900362968445 \n","Iteration: 15900, Loss: 0.6701900362968445 \n","Iteration: 16000, Loss: 0.6701900362968445 \n","Iteration: 16100, Loss: 0.6701900362968445 \n","Iteration: 16200, Loss: 0.6701900362968445 \n","Iteration: 16300, Loss: 0.6701900362968445 \n","Iteration: 16400, Loss: 0.6701900362968445 \n","Iteration: 16500, Loss: 0.6701900362968445 \n","Iteration: 16600, Loss: 0.6701900362968445 \n","Iteration: 16700, Loss: 0.6701900362968445 \n","Iteration: 16800, Loss: 0.6701900362968445 \n","Iteration: 16900, Loss: 0.6701900362968445 \n","Iteration: 17000, Loss: 0.6701900362968445 \n","Iteration: 17100, Loss: 0.6701900362968445 \n","Iteration: 17200, Loss: 0.6701900362968445 \n","Iteration: 17300, Loss: 0.6701900362968445 \n","Iteration: 17400, Loss: 0.6701900362968445 \n","Iteration: 17500, Loss: 0.6701900362968445 \n","Iteration: 17600, Loss: 0.6701900362968445 \n","Iteration: 17700, Loss: 0.6701900362968445 \n","Iteration: 17800, Loss: 0.6701900362968445 \n","Iteration: 17900, Loss: 0.6701900362968445 \n","Iteration: 18000, Loss: 0.6701900362968445 \n","Iteration: 18100, Loss: 0.6701900362968445 \n","Iteration: 18200, Loss: 0.6701900362968445 \n","Iteration: 18300, Loss: 0.6701900362968445 \n","Iteration: 18400, Loss: 0.6701900362968445 \n","Iteration: 18500, Loss: 0.6701900362968445 \n","Iteration: 18600, Loss: 0.6701900362968445 \n","Iteration: 18700, Loss: 0.6701900362968445 \n","Iteration: 18800, Loss: 0.6701900362968445 \n","Iteration: 18900, Loss: 0.6701900362968445 \n","Iteration: 19000, Loss: 0.6701900362968445 \n","Iteration: 19100, Loss: 0.6701900362968445 \n","Iteration: 19200, Loss: 0.6701900362968445 \n","Iteration: 19300, Loss: 0.6701900362968445 \n","Iteration: 19400, Loss: 0.6701900362968445 \n","Iteration: 19500, Loss: 0.6701900362968445 \n","Iteration: 19600, Loss: 0.6701900362968445 \n","Iteration: 19700, Loss: 0.6701900362968445 \n","Iteration: 19800, Loss: 0.6701900362968445 \n","Iteration: 19900, Loss: 0.6701900362968445 \n","\n","Number of iterations: 20000\n","Final loss value: 0.6701900362968445\n","Final error: 2.5862627029418945\n","The solution is: A = [1.9584198 1.6891851]\n"]}]},{"cell_type":"markdown","metadata":{"id":"W8ZrxTACsOp3"},"source":["---\n","### Problem 2 (50 points) \n","\n","Solve the following problem using Bayesian Optimization:\n","$$\n","    \\min_{x_1, x_2} \\quad \\left(4-2.1x_1^2 + \\frac{x_1^4}{3}\\right)x_1^2 + x_1x_2 + \\left(-4 + 4x_2^2\\right)x_2^2,\n","$$\n","for $x_1 \\in [-3,3]$ and $x_2 \\in [-2,2]$. A tutorial on Bayesian Optimization can be found [here](https://thuijskens.github.io/2016/12/29/bayesian-optimisation/).\n","\n"],"id":"W8ZrxTACsOp3"},{"cell_type":"markdown","metadata":{"id":"iTI-ucGesukH"},"source":["#### Solution \n","\n"],"id":"iTI-ucGesukH"},{"cell_type":"code","metadata":{"id":"divine-setup"},"source":[""],"id":"divine-setup","execution_count":null,"outputs":[]}]}