{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"DesignOptimization_HW_2.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"DY-XwFeWp7Eg"},"source":["# Theory/Computation Problems\n","\n","### Problem 1 (20 points) \n","Show that the stationary point (zero gradient) of the function\n","$$\n","\\begin{aligned}\n","    f=2x_{1}^{2} - 4x_1 x_2+ 1.5x^{2}_{2}+ x_2\n","\\end{aligned}\n","$$\n","is a saddle (with indefinite Hessian). Find the directions of downslopes away from the saddle. Hint: Use Taylor's expansion at the saddle point. Find directions that reduce $f$."],"id":"DY-XwFeWp7Eg"},{"cell_type":"markdown","metadata":{"id":"wMnZVqSHqRBx"},"source":["#### Solution \n","\n","$$f(x)=2x_1^2 - 4x_1 x_2+ 1.5x^2_2+ x_2$$\n","\n","The gradient of the function $f(x)$ is: \n","\n","$$g(x)=\\nabla f(x)=\n","    \\begin{bmatrix}\n","    \\frac{\\partial f}{\\partial x_1}\\\\\\\\\n","    \\frac{\\partial f}{\\partial x_2}\n","    \\end{bmatrix}\n","    =\n","    \\begin{bmatrix}\n","    4x_1-4x_2\\\\\\\\\n","    -4x_1+3x_2+1\n","    \\end{bmatrix}\n","$$\n","\n","Let $x^\\star$ be a stationary point of $f(x)$. Then, $x^\\star$ satisfies the\n","equation $g(x^\\star)=0$.\n","\n","$$\\Rightarrow \n","    g(x^\\star)=\n","    \\begin{bmatrix}\n","    4x_1-4x_2\\\\\\\\\n","    -4x_1+3x_2+1\n","    \\end{bmatrix}\n","    =\n","    \\begin{bmatrix}\n","    0\\\\\\\\\n","    0\n","    \\end{bmatrix}\n","$$\n","\n","<br>\n","$$\\Rightarrow \n","  \\begin{equation}\n","  \\left\\{\\begin{aligned}\n","    4x_1-4x_2=0\\\\\n","    -4x_1+3x_2+1=0\n","  \\end{aligned}\n","  \\right.\\end{equation} \n","$$\n","\n","<br>\n","$$\\Rightarrow \n","  \\begin{equation}\n","  \\left\\{\\begin{aligned}\n","    x_1 &=x_2\\\\\n","    -4x_1+3x_1+1 &=0\n","  \\end{aligned}\n","  \\right.\\end{equation} \n","$$\n","\n","<br>\n","$$\\begin{aligned}\n","  \\Rightarrow x_1=1=x_2\\\\\n","  \\Rightarrow x^\\star=\n","  \\begin{bmatrix}\n","    1\\\\\n","    1\n","  \\end{bmatrix}\\end{aligned}\n","$$\n","\n","\n","<br> \n","The Hessian matrix of $f(x)$ is:\n","\n","$$\\begin{aligned}\n","  H(x)=\\nabla ^2f(x)=\n","  \\begin{bmatrix}\n","    \\frac{\\partial ^2 f}{\\partial x_1^2} && \\frac{\\partial ^2 f}{\\partial x_1\\partial x_2}\\\\\\\\\n","    \\frac{\\partial ^2 f}{\\partial x_2\\partial x_1} && \\frac{\\partial ^2 f}{\\partial x_2^2}\n","  \\end{bmatrix}\n","  =\n","  \\begin{bmatrix}\n","    4 && -4\\\\\\\\\n","    -4 && 3\n","  \\end{bmatrix}\\end{aligned}\n","$$\n","\n","and $H(x^\\star)= \n","  \\begin{bmatrix}\n","    4 && -4\\\\\\\\\n","    -4 && 3\n","  \\end{bmatrix}$.\n","\n","<br> \n","We have that: <br> \n","$$|H(x)|=\\lambda_1 \\cdot \\lambda_2$$ \n","and also: <br> \n"," $$\\begin{aligned}\n","    |H(x)|=3\\cdot 4-(-4)\\cdot (-4)=12-16=-4<0\\end{aligned}\n"," $$\n","\n","$\\Rightarrow \\lambda_1,\\lambda_2$ have opposite signs. Hence, the matrix\n","$H(x)$ is indefinite and therefore, the stationary point is a **saddle point**. \n","\n","\n","<br />\n","The 2nd-order Taylor Series approximation of the function is:\n","$$f(x)\\approx f(x^\\star)+g^T(x^\\star)(x-x^\\star)+\\frac{1}{2}(x-x^\\star)^TH(x^\\star)(x-x^\\star)$$\n","\n","where \n","\n","$f(x^\\star)=2\\cdot 1^2-4\\cdot 1\\cdot 1+1.5\\cdot 1^2+1=0.5$\n","<br>\n","$g(x^\\star)=0$\n","\n","$$\\Rightarrow f(x)\\approx 0.5+\\frac{1}{2} \n","  \\begin{bmatrix}\n","    x_1-1 && x_2-1\n","  \\end{bmatrix} \n","  \\begin{bmatrix}\n","    4 && -4\\\\\\\\\n","    -4 && 3\n","  \\end{bmatrix}\n","  \\begin{bmatrix}\n","    x_1-1 \\\\\\\\\n","    x_2-1\n","  \\end{bmatrix}\n","  =2x_1^2 - 4x_1 x_2+ 1.5x^2_2+ x_2\n","$$\n","\n","\n","The eigenvalues of the Hessian are:\n"],"id":"wMnZVqSHqRBx"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vvUsr__GRlWl","executionInfo":{"status":"ok","timestamp":1631355160608,"user_tz":-180,"elapsed":370,"user":{"displayName":"Elena Oikonomou","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdjieaR8SwyW3oT5Gj_-eTKl6SVjziMwjwjye6=s64","userId":"03571600125738632827"}},"outputId":"56ab54fa-a89b-4612-f45b-0a7eec64ae1c"},"source":["import numpy as np\n","from numpy import linalg as LA\n","\n","H = np.array([[4, -4], [-4, 3]])  # Hessian\n","w,v = LA.eig(H)                   # Eigenvalues and eigenvectors \n","print(\"The eigenvalues are: {}\".format(w))\n","print(\"The eigenvectors are:\\n {}\".format(v))"],"id":"vvUsr__GRlWl","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["The eigenvalues are: [ 7.53112887 -0.53112887]\n","The eigenvectors are:\n"," [[ 0.74967818  0.66180256]\n"," [-0.66180256  0.74967818]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"v-fMjQEPTNZL"},"source":["$\\lambda_1=7.53112887, \\lambda_2=-0.53112887$\n","\n","The direction of downslope, where the function curves down, is the principal direction which corresponds to the negative eigenvalue. This is:\n","\n","$$v_2=\\begin{bmatrix}\n","    -0.66180256\\\\\\\\\n","    0.74967818\n","  \\end{bmatrix}$$"],"id":"v-fMjQEPTNZL"},{"cell_type":"markdown","metadata":{"id":"touched-logic"},"source":["\n","\n","### Problem 2 (50 points) \n","\n","* (10 points) Find the point in the plane $x_1+2x_2+3x_3=1$ in $\\mathbb{R}^3$ that is nearest to the point $(-1,0,1)^T$. Is this a convex problem? Hint: Convert the problem into an unconstrained problem using $x_1+2x_2+3x_3=1$.\n","\n","* (40 points) Implement the gradient descent and Newton's algorithm for solving the problem. Attach your codes along with a short summary including (1) the initial points tested, (2) corresponding solutions, (3) a log-linear convergence plot.\n","\n","### Problem 3 (10 points) \n","Let $f(x)$ and $g(x)$ be two convex functions defined on the convex set $\\mathcal{X}$. \n","* (5 points) Prove that $af(x)+bg(x)$ is convex for $a>0$ and $b>0$. \n","* (5 points) In what conditions will $f(g(x))$ be convex?\n","\n","### Problem 4 (bonus 10 points)\n","Show that $f({\\bf x}_1) \\geq f(\\textbf{x}_0) + \n","    \\textbf{g}_{\\textbf{x}_0}^T(\\textbf{x}_1-\\textbf{x}_0)$ for a convex function $f(\\textbf{x}): \\mathcal{X} \\rightarrow \\mathbb{R}$ and for $\\textbf{x}_0$, $\\textbf{x}_1 \\in \\mathcal{X} \n","$. "],"id":"touched-logic"},{"cell_type":"markdown","metadata":{"id":"collected-carbon"},"source":["# Design Problems\n","\n","### Problem 5 (20 points) \n","Consider an illumination problem: There are $n$ lamps and $m$ mirrors fixed to the ground. The target reflection intensity level is $I_t$. The actual reflection intensity level on the $k$th mirror can be computed as $\\textbf{a}_k^T \\textbf{p}$, where $\\textbf{a}_k$ is given by the distances between all lamps to the mirror, and $\\textbf{p}:=[p_1,...,p_n]^T$ are the power output of the lamps. The objective is to keep the actual intensity levels as close to the target as possible by tuning the power output $\\textbf{p}$.\n","\n","* (5 points) Formulate this problem as an optimization problem. \n","* (5 points) Is your problem convex?\n","* (5 points) If we require the overall power output of any of the $n$ lamps to be less than $p^*$, will the problem have a unique solution?\n","* (5 points) If we require no more than half of the lamps to be switched on, will the problem have a unique solution?"],"id":"collected-carbon"},{"cell_type":"markdown","metadata":{"id":"moderate-twins"},"source":["# Note\n","\n","For this homework, you may want to attach sketches as means to explain your ideas. Here is how you can attach images.\n","\n","![everly1](img/everly7.jpg)"],"id":"moderate-twins"}]}