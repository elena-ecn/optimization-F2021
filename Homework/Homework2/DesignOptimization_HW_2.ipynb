{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"DesignOptimization_HW_2.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"DY-XwFeWp7Eg"},"source":["# Theory/Computation Problems\n","\n","### Problem 1 (20 points) \n","Show that the stationary point (zero gradient) of the function\n","$$\n","\\begin{aligned}\n","    f=2x_{1}^{2} - 4x_1 x_2+ 1.5x^{2}_{2}+ x_2\n","\\end{aligned}\n","$$\n","is a saddle (with indefinite Hessian). Find the directions of downslopes away from the saddle. Hint: Use Taylor's expansion at the saddle point. Find directions that reduce $f$."],"id":"DY-XwFeWp7Eg"},{"cell_type":"markdown","metadata":{"id":"wMnZVqSHqRBx"},"source":["#### Solution \n","\n","$$f(x)=2x_1^2 - 4x_1 x_2+ 1.5x^2_2+ x_2$$\n","\n","The gradient of the function $f(x)$ is: \n","\n","$$g(x)=\\nabla f(x)=\n","    \\begin{bmatrix}\n","    \\frac{\\partial f}{\\partial x_1}\\\\\\\\\n","    \\frac{\\partial f}{\\partial x_2}\n","    \\end{bmatrix}\n","    =\n","    \\begin{bmatrix}\n","    4x_1-4x_2\\\\\\\\\n","    -4x_1+3x_2+1\n","    \\end{bmatrix}\n","$$\n","\n","Let $x^\\star$ be a stationary point of $f(x)$. Then, $x^\\star$ satisfies the\n","equation $g(x^\\star)=0$.\n","\n","$$\\Rightarrow \n","    g(x^\\star)=\n","    \\begin{bmatrix}\n","    4x_1-4x_2\\\\\\\\\n","    -4x_1+3x_2+1\n","    \\end{bmatrix}\n","    =\n","    \\begin{bmatrix}\n","    0\\\\\\\\\n","    0\n","    \\end{bmatrix}\n","$$\n","\n","<br>\n","$$\\Rightarrow \n","  \\begin{equation*}\n","  \\left\\{\\begin{aligned}\n","    4x_1-4x_2=0\\\\\n","    -4x_1+3x_2+1=0\n","  \\end{aligned}\n","  \\right.\\end{equation*} \n","$$\n","\n","<br>\n","$$\\Rightarrow \n","  \\begin{equation*}\n","  \\left\\{\\begin{aligned}\n","    x_1 &=x_2\\\\\n","    -4x_1+3x_1+1 &=0\n","  \\end{aligned}\n","  \\right.\\end{equation*} \n","$$\n","\n","<br>\n","$$\\begin{aligned}\n","  \\Rightarrow x_1=1=x_2\\\\\n","  \\Rightarrow x^\\star=\n","  \\begin{bmatrix}\n","    1\\\\\n","    1\n","  \\end{bmatrix}\\end{aligned}\n","$$\n","\n","\n","<br> \n","The Hessian matrix of $f(x)$ is:\n","\n","$$\\begin{aligned}\n","  H(x)=\\nabla ^2f(x)=\n","  \\begin{bmatrix}\n","    \\frac{\\partial ^2 f}{\\partial x_1^2} && \\frac{\\partial ^2 f}{\\partial x_1\\partial x_2}\\\\\\\\\n","    \\frac{\\partial ^2 f}{\\partial x_2\\partial x_1} && \\frac{\\partial ^2 f}{\\partial x_2^2}\n","  \\end{bmatrix}\n","  =\n","  \\begin{bmatrix}\n","    4 && -4\\\\\\\\\n","    -4 && 3\n","  \\end{bmatrix}\\end{aligned}\n","$$\n","\n","and $H(x^\\star)= \n","  \\begin{bmatrix}\n","    4 && -4\\\\\\\\\n","    -4 && 3\n","  \\end{bmatrix}$.\n","\n","<br> \n","We have that: <br> \n","$$|H(x)|=\\lambda_1 \\cdot \\lambda_2$$ \n","and also: <br> \n"," $$\\begin{aligned}\n","    |H(x)|=3\\cdot 4-(-4)\\cdot (-4)=12-16=-4<0\n","    \\end{aligned}\n"," $$\n","\n","$\\Rightarrow \\lambda_1,\\lambda_2$ have opposite signs. Hence, the matrix\n","$H(x)$ is indefinite and therefore, the stationary point is a **saddle point**. \n","\n","\n","<br />\n","The 2nd-order Taylor Series approximation of the function is:\n","$$f(x)\\approx f(x^\\star)+g^T(x^\\star)(x-x^\\star)+\\frac{1}{2}(x-x^\\star)^TH(x^\\star)(x-x^\\star)$$\n","\n","where \n","\n","$f(x^\\star)=2\\cdot 1^2-4\\cdot 1\\cdot 1+1.5\\cdot 1^2+1=0.5$\n","<br>\n","$g(x^\\star)=0$\n","\n","$$\\Rightarrow f(x)\\approx 0.5+\\frac{1}{2} \n","  \\begin{bmatrix}\n","    x_1-1 && x_2-1\n","  \\end{bmatrix} \n","  \\begin{bmatrix}\n","    4 && -4\\\\\\\\\n","    -4 && 3\n","  \\end{bmatrix}\n","  \\begin{bmatrix}\n","    x_1-1 \\\\\\\\\n","    x_2-1\n","  \\end{bmatrix}\n","  =2x_1^2 - 4x_1 x_2+ 1.5x^2_2+ x_2\n","$$\n","\n","\n","At the saddle point, in the directions that $f$ reduces, we have that \n","$f(x)< f(x^\\star)$.\n","\n","$$\\Rightarrow f(x)-f(x^\\star)< 0 $$\n","$$\\Rightarrow \\frac{1}{2}(x-x^\\star)^TH(x^\\star)(x-x^\\star)< 0 $$\n","$$\\Rightarrow (x-x^\\star)^TH(x^\\star)(x-x^\\star)< 0 $$\n","$$\\Rightarrow 4x_1^2 - 8x_1 x_2+ 3x^2_2+ 2x_2 -1< 0 $$\n","$$\\Rightarrow (2x_1-3x_2+1)(2x_1-x_2-1)< 0 $$\n","\n","\n","which means that $f$ reduces when:\n","\n","$$(2x_1-3x_2+1)>0  \\ and\\ (2x_1-x_2-1)< 0 $$\n","or\n","$$(2x_1-3x_2+1)< 0 \\ and\\ (2x_1-x_2-1)> 0 $$\n","\n","\n","\n","More specifically, the eigenvalues of the Hessian are:\n"],"id":"wMnZVqSHqRBx"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vvUsr__GRlWl","executionInfo":{"status":"ok","timestamp":1631362192765,"user_tz":-180,"elapsed":360,"user":{"displayName":"Elena Oikonomou","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdjieaR8SwyW3oT5Gj_-eTKl6SVjziMwjwjye6=s64","userId":"03571600125738632827"}},"outputId":"4ae48e7b-1d15-4e61-9b75-cb43b8985933"},"source":["import numpy as np\n","from numpy import linalg as LA\n","\n","H = np.array([[4, -4], [-4, 3]])  # Hessian\n","w,v = LA.eig(H)                   # Eigenvalues and eigenvectors \n","print(\"The eigenvalues are: {}\".format(w))\n","print(\"The eigenvectors are:\\n {}\".format(v))"],"id":"vvUsr__GRlWl","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The eigenvalues are: [ 7.53112887 -0.53112887]\n","The eigenvectors are:\n"," [[ 0.74967818  0.66180256]\n"," [-0.66180256  0.74967818]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"v-fMjQEPTNZL"},"source":["$\\lambda_1=7.53112887, \\lambda_2=-0.53112887$\n","\n","The direction of downslope, where the function curves down, is the principal direction which corresponds to the negative eigenvalue. This is:\n","\n","$$v_2=\\begin{bmatrix}\n","    -0.66180256\\\\\\\\\n","    0.74967818\n","  \\end{bmatrix}$$"],"id":"v-fMjQEPTNZL"},{"cell_type":"markdown","metadata":{"id":"xXLxSnH8dLAj"},"source":["### Problem 2 (50 points) \n","\n","* (10 points) Find the point in the plane $x_1+2x_2+3x_3=1$ in $\\mathbb{R}^3$ that is nearest to the point $(-1,0,1)^T$. Is this a convex problem? Hint: Convert the problem into an unconstrained problem using $x_1+2x_2+3x_3=1$.\n","\n","* (40 points) Implement the gradient descent and Newton's algorithm for solving the problem. Attach your codes along with a short summary including (1) the initial points tested, (2) corresponding solutions, (3) a log-linear convergence plot."],"id":"xXLxSnH8dLAj"},{"cell_type":"markdown","metadata":{"id":"Z7bocA9ydSfW"},"source":["#### Solution\n","\n","i) If we let \n","$w=\\begin{bmatrix}\n","    x_1 & x_2 & x_3\n","  \\end{bmatrix}^T$\n","be any point on the plane and \n","$x_0=\\begin{bmatrix}\n","    -1 & 0 & 1\n","  \\end{bmatrix}^T$\n","be the given point of interest, then the vector\n","\n","$$z=\\begin{bmatrix}\n","    x_1-(-1)\\\\\\\\ \n","    x_2-0\\\\\\\\\n","    x_3-1\n","  \\end{bmatrix}\n","  =\n","  \\begin{bmatrix}\n","    x_1+1\\\\\\\\ \n","    x_2\\\\\\\\\n","    x_3-1\n","  \\end{bmatrix}\n","$$\n","\n","denotes the distance between them. The point $w$ that is nearest to $x_0$ is the one with the smallest distance, i.e. whose vector $z$ length is minimum.\n","The length of the vector $z$ is:\n","$$|z|=\\sqrt{(x_1+1)^2+(x_2)^2+(x_3-1)^2}$$\n","<br>\n","Therefore, we need to find the solution to the following minimization problem: \n","\n","$$\n","\\begin{aligned}\n","&\\underset{x_1,x_2,x_3}{\\text{minimize:}} && (x_1+1)^2+(x_2)^2+(x_3-1)^2 \\\\\\\\\n","&\\text{subject to:} && x_1+2x_2+3x_3=1 \\\\\\\\\n","&&& x_i \\in \\mathbb{R}, \\ i=1,2,3\n","\\end{aligned}\n","$$\n","\n","<br>\n","If we let $x_1=-2x_2-3x_3+1$, we have that:\n","$$\n","(x_1+1)^2+(x_2)^2+(x_3-1)^2 = (2-2x_2-3x_3)^2+(x_2)^2+(x_3-1)^2\n","$$\n","\n","and we can convert the problem to the following unconstrained optimization problem:\n","\n","$$\n","\\underset{x_2,x_3}{\\text{minimize: }}  f(x)=(2-2x_2-3x_3)^2+(x_2)^2+(x_3-1)^2 \n","$$\n","\n","The solution to the minimization problem is:"],"id":"Z7bocA9ydSfW"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Oj4XkGisAm-b","executionInfo":{"status":"ok","timestamp":1631474628760,"user_tz":-180,"elapsed":360,"user":{"displayName":"Elena Oikonomou","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdjieaR8SwyW3oT5Gj_-eTKl6SVjziMwjwjye6=s64","userId":"03571600125738632827"}},"outputId":"21bf8178-4f3a-4747-944d-3220e7f8bc03"},"source":["from scipy.optimize import minimize\n","\n","# Objective function\n","fun = lambda x: (2-2*x[0]-3*x[1])**2 + (x[0])**2 + (x[1]-1)**2\n","\n","# Initial guess\n","x0 = (1, 0)\n","\n","# Optimization Result\n","res = minimize(fun, x0, method='SLSQP')\n","\n","# Function value\n","print('Function value: f = {}'.format(res.fun))\n","\n","# Variable values \n","print('Variable values:\\nx2, x3 = {}, {}'.format(res.x[0], res.x[1]))\n","\n","# Compute x1\n","x1=-2*res.x[0]-3*res.x[1]+1\n","print(\"x1 = {}\".format(x1))"],"id":"Oj4XkGisAm-b","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Function value: f = 0.07142857142857148\n","Variable values:\n","x2, x3 = -0.14285713906428532, 0.7857142823269022\n","x1 = -1.071428568852136\n"]}]},{"cell_type":"markdown","metadata":{"id":"tujTkKJ8B8Zz"},"source":["Therefore, the point in the plane that is nearest to $x_0$ is: \n","$w=\\begin{bmatrix}\n","    x_1 & x_2 & x_3\n","  \\end{bmatrix}^T\n","  =\n","  \\begin{bmatrix}\n","    -1.071428 & -0.142857 & 0.785714\n","  \\end{bmatrix}^T $\n","\n","The gradient of $f(x)$ is:\n","$$g(x)=\\nabla f(x)=\n","    \\begin{bmatrix}\n","    10x_2+12x_3-8\\\\\\\\\n","    12x_2+20x_3-14\n","    \\end{bmatrix}\n","$$\n","\n","The Hessian is:\n","$$\\begin{aligned}\n","  H(x)=\\nabla ^2f(x)=\n","  \\begin{bmatrix}\n","    10 && 12\\\\\\\\\n","    12 && 20\n","  \\end{bmatrix}\\end{aligned}\n","$$\n"],"id":"tujTkKJ8B8Zz"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vlBbs3WsHl-C","executionInfo":{"status":"ok","timestamp":1631368191470,"user_tz":-180,"elapsed":443,"user":{"displayName":"Elena Oikonomou","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdjieaR8SwyW3oT5Gj_-eTKl6SVjziMwjwjye6=s64","userId":"03571600125738632827"}},"outputId":"365854c0-b097-42cc-b3d8-f3aedb4b94c9"},"source":["import numpy as np\n","from numpy import linalg as LA\n","\n","H = np.array([[10, 12], [12, 20]])  # Hessian\n","w,v = LA.eig(H)                     # Eigenvalues and eigenvectors \n","print(\"The eigenvalues are: {}\".format(w))"],"id":"vlBbs3WsHl-C","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The eigenvalues are: [ 2. 28.]\n"]}]},{"cell_type":"markdown","metadata":{"id":"m7CZHzCcH1k4"},"source":["The eigenvalues are all positive, hence the Hessian is positive definite everywhere. Therefore, $f(x)$ and thus the unconstrained optimization problem are convex.\n","\n","<br>\n","ii)"],"id":"m7CZHzCcH1k4"},{"cell_type":"markdown","metadata":{"id":"touched-logic"},"source":["\n","\n","### Problem 3 (10 points) \n","Let $f(x)$ and $g(x)$ be two convex functions defined on the convex set $\\mathcal{X}$. \n","* (5 points) Prove that $af(x)+bg(x)$ is convex for $a>0$ and $b>0$. \n","* (5 points) In what conditions will $f(g(x))$ be convex?\n","\n"],"id":"touched-logic"},{"cell_type":"markdown","metadata":{"id":"3MX8v4te9lWm"},"source":["#### Solution\n","\n","i) Let us define a function $H(x)=af(x)+bg(x)$. In order to prove that $H(x)$ is convex, we must show that it satisfies the definition of convexity. Therefore, we must show that:\n","\n","$$H(\\lambda x_1+(1-\\lambda)x_2) \\le \\lambda H(x_1)+(1-\\lambda)H(x_2)\n","$$\n","\n","Let $x_1,x_2\\in \\mathcal{X}$. Then by the definition of convexity of $f,g$ we have that $\\forall \\lambda\\in[0,1]$:\n","\n","$$f(\\lambda x_1+(1-\\lambda)x_2)\\le \\lambda f(x_1)+(1-\\lambda)f(x_2)\n","$$\n","\n","and \n","\n","$$g(\\lambda x_1+(1-\\lambda)x_2)\\le \\lambda g(x_1)+(1-\\lambda)g(x_2)\n","$$\n","\n","For $a,b>0$, if we multiply both sides of the inequalities with $a,b$ respectively, we get:\n","\n","$$af(\\lambda x_1+(1-\\lambda)x_2)\\le a\\lambda f(x_1)+a(1-\\lambda)f(x_2)\n","$$\n","<br>\n","$$bg(\\lambda x_1+(1-\\lambda)x_2)\\le b\\lambda g(x_1)+b(1-\\lambda)g(x_2)\n","$$\n","\n","Adding the two inequalities produces:\n","\n","$$af(\\lambda x_1+(1-\\lambda)x_2)+bg(\\lambda x_1+(1-\\lambda)x_2) \\le a\\lambda f(x_1)+a(1-\\lambda)f(x_2) +b\\lambda g(x_1)+b(1-\\lambda)g(x_2) \n","= \\lambda (af(x_1)+bg(x_1))+(1-\\lambda)(af(x_2)+bg(x_2))\n","$$\n","<br>\n","$$\\Rightarrow H(\\lambda x_1+(1-\\lambda)x_2) \\le \\lambda H(x_1)+(1-\\lambda)H(x_2)\n","$$\n","<br>\n","Then, for $a,b>0$, by definition of convexity, $H(x)=af(x)+bg(x)$ is convex.\n","\n","<br>\n","ii) Let $h(x)=f(g(x))$. If $f,g$ twice differentiable functions:\n","\n","$$h'(x)=f'(g(x))\\cdot g'(x)\n","$$\n","<br>\n","$$h''(x)=f''(g(x))\\cdot (g'(x))^2+f'(g(x))\\cdot g''(x)\n","$$\n","\n","Therefore, for f(g(x)) to be convex, it must be $h''(x) \\ge 0$."],"id":"3MX8v4te9lWm"},{"cell_type":"markdown","metadata":{"id":"lbPt7PFE9dGk"},"source":["### Problem 4 (bonus 10 points)\n","Show that $f({\\bf x}_1) \\geq f(\\textbf{x}_0) + \n","    \\textbf{g}_{\\textbf{x}_0}^T(\\textbf{x}_1-\\textbf{x}_0)$ for a convex function $f(\\textbf{x}): \\mathcal{X} \\rightarrow \\mathbb{R}$ and for $\\textbf{x}_0$, $\\textbf{x}_1 \\in \\mathcal{X} \n","$. "],"id":"lbPt7PFE9dGk"},{"cell_type":"markdown","metadata":{"id":"XwFq4Gfi9pS1"},"source":["#### Solution\n","\n","If $f$ is convex, we have:\n","\n","$$f(\\lambda x_0+(1-\\lambda)x_1) \\le \\lambda f(x_0)+(1-\\lambda)f(x_1)\n","$$\n","<br>\n","$$\\Rightarrow \\frac{f(\\lambda x_0+(1-\\lambda)x_1)}{\\lambda} \\le f(x_0)+\\frac{f(x_1)}{\\lambda}-f(x_1)\n","$$\n","<br>\n","$$\\Rightarrow f(x_0) \\ge f(x_1) +\\frac{f(\\lambda x_0+(1-\\lambda)x_1)-f(x_1)}{\\lambda}\n","$$\n","For $\\lambda \\to 0$,\n","<br>\n","$$f(x_0) \\ge f(x_1) +\\lim_{\\lambda\\to 0}\\frac{f(\\lambda x_0+(1-\\lambda)x_1)-f(x_1)}{\\lambda (x_0-x_1)}\\cdot (x_0-x_1)\n","$$\n","\n","By definition of the derivative, for $h=\\lambda (x_0-x_1)$, we have that:\n","\n","$$f'(x_1)=g(x_1)=\\lim_{h\\to 0} \\frac{f(x_1+h)-f(x_1)}{h}=\\lim_{\\lambda\\to 0}\\frac{f(\\lambda x_0+(1-\\lambda)x_1)-f(x_1)}{\\lambda (x_0-x_1)}\n","$$\n","<br>\n","$$\\Rightarrow f(x_0) \\ge f(x_1) +g^T(x_1)\\cdot (x_0-x_1)\n","\\quad \\square\n","$$"],"id":"XwFq4Gfi9pS1"},{"cell_type":"markdown","metadata":{"id":"collected-carbon"},"source":["# Design Problems\n","\n","### Problem 5 (20 points) \n","Consider an illumination problem: There are $n$ lamps and $m$ mirrors fixed to the ground. The target reflection intensity level is $I_t$. The actual reflection intensity level on the $k$th mirror can be computed as $\\textbf{a}_k^T \\textbf{p}$, where $\\textbf{a}_k$ is given by the distances between all lamps to the mirror, and $\\textbf{p}:=[p_1,...,p_n]^T$ are the power output of the lamps. The objective is to keep the actual intensity levels as close to the target as possible by tuning the power output $\\textbf{p}$.\n","\n","* (5 points) Formulate this problem as an optimization problem. \n","* (5 points) Is your problem convex?\n","* (5 points) If we require the overall power output of any of the $n$ lamps to be less than $p^*$, will the problem have a unique solution?\n","* (5 points) If we require no more than half of the lamps to be switched on, will the problem have a unique solution?"],"id":"collected-carbon"},{"cell_type":"markdown","metadata":{"id":"OQxWuj6nNR99"},"source":["#### Solution\n","\n","i)\n","The reflection intensity level on the k-th mirror is $I_k=a_K^T\\cdot P$. We want to minimize the error $(I_k-I_t)$ for all mirrors. Therefore, the problem can be formulated as follows:\n","\n","<br>\n","$$\n","  \\begin{aligned}\n","  &\\underset{\\{P_j\\}_{j=1}^n}{\\text{minimize:}} && \\sum_{k=1}^{m}(a_K^T\\cdot P-I_t)^2 \\\\\\\\\n","  &\\text{subject to:} && 0\\le P_j \\le P_{max} \\ , \\forall j=1,\\dotsc,n\n","  \\end{aligned}\n","$$\n","\n","where $P_{max}$ is the maximum power output each lamp can produce and $I_k=a_K^T\\cdot P=\\sum_{j=1}^{n}(a_{Kj}\\cdot P_j)$.\n","\n","ii)\n","The feasible domain represents a hypercube. Since the hypercube is convex, the feasible domain is also convex. <br>\n","\n","The objective function is:\n","$$\n","f(P)=\\sum_{k=1}^{m}(a_K^T\\cdot P-I_t)^2=\\sum_{k=1}^{m}(P^Ta_K\\cdot a_K^T P-2a_K^TPI_t+I_t^2)\n","$$\n","\n","If we define:\n","$$\\alpha=\\sum_{k=1}^{m}I_t^2 \\ , \\ \\beta=-2\\sum_{k=1}^{m}a_K I_t \\ , \\ C=\\sum_{k=1}^{m}a_Ka_K^T\n","$$\n","\n","the objective function becomes:\n","$$\n","f(P)=\\alpha +\\beta ^TP+P^TCP\n","$$\n","\n","The gradient of the objective function is then:\n","$$\n","\\nabla f(P)=\\beta+2CP\n","$$\n","\n","and the Hessian:\n","$$\n","H(P)=2C\n","$$\n","\n","Since matric C is positive semi-definite (quadratic terms), the Hessian is also psd. Therefore, the objective function is convex.\n","\n","Both the objective function and the feasible domain are convex, thus the optimization problem is convex.\n","\n","<br>\n","iii)\n","If we require the overall power output of any of the n lamps to be less than $P^\\star$, we are adding the following constraint:\n","\n","$$\\sum_{j=1}^{n}P_j \\le P^\\star\n","$$\n","\n","This constraint is linear in P, thus convex. In order to have a unique solution, the objective function must be strictly convex.\n","<br>\n","iv)\n","If we have two lamps (n=2), then 1 lamp can be on and 1 lamp can be off. But then, the feasible domain will not be convex as can be seen from the following figure:\n","<br>\n","![feas_domain](img/5d.jpg)\n","\n","Therefore, the problem will not have a unique solution.\n"],"id":"OQxWuj6nNR99"},{"cell_type":"markdown","metadata":{"id":"moderate-twins"},"source":["# Note\n","\n","For this homework, you may want to attach sketches as means to explain your ideas. Here is how you can attach images.\n","\n"],"id":"moderate-twins"}]}